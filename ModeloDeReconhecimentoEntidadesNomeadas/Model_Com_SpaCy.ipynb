{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "import glob\n",
    "import csv\n",
    "from spacy.training import offsets_to_biluo_tags\n",
    "from spacy.training import biluo_tags_to_spans\n",
    "from spacy.training import offsets_to_biluo_tags, biluo_tags_to_offsets, biluo_tags_to_spans\n",
    "import itertools\n",
    "\n",
    "# importações para o bloco de código do treinamento do modelo de NER\n",
    "import random\n",
    "from spacy.util import minibatch, compounding\n",
    "from pathlib import Path\n",
    "from spacy.tokens import Doc\n",
    "from spacy.training import Example\n",
    "import warnings\n",
    "import datetime as date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregando Dados Rotulados Manualmente Para Treinamento do Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carrega os dados no formato .csv e cria um dataframe a partir deles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSet = pd.read_csv(\"/home/arthur/Documentos/IC/git/ProjetoMestrado/DataSets/Dataset_Padrao_IOB_Versao4.csv\", usecols=['Palavras', 'Rotulo', 'Sentenca', 'Inicio', 'Fim'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carrega a base utilizada para rotulação manual das categorias. Utilizada para extrair as setenças a que cada token rotulado pertence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de linhas da base: 14628\n"
     ]
    }
   ],
   "source": [
    "with open(\"/home/arthur/Documentos/IC/git/ProjetoMestrado/DataSets/Dados_Utilizados_Para_RotulacaoManual.txt\", \"r\", encoding='utf-8') as file:\n",
    "    textos = file.read().splitlines()\n",
    "print(\"Quantidade de linhas da base:\" , len(textos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Código para converter os dados que estão no formato IOB para o formato do spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11294"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sMarker = 0 #Marcador que referência a qual setença a entidade pertence, ou seja, serve pra pegar o texto da referencia dos tokens no dataset\n",
    "s = textos[sMarker]\n",
    "\n",
    "TRAIN_DATA = []\n",
    "entities = []\n",
    "for index, row in dataSet.iterrows():\n",
    "    \n",
    "    if(row['Sentenca']-1 != sMarker): # Nova sentença\n",
    "        if entities:\n",
    "            TRAIN_DATA.append((s, {\"entities\": [tuple(e) for e in entities]})) # Salva sentença anterior\n",
    "        entities = [] # esvazia entidades\n",
    "        sMarker = row['Sentenca']-1 # atualiza o marcador de sentença\n",
    "        \n",
    "        if (sMarker < len(textos)): # Limite de textos\n",
    "            s = textos[sMarker]\n",
    "\n",
    "    if(row['Rotulo'][0] == 'B'):\n",
    "         entities.append([row['Inicio'], row['Fim'], row['Rotulo'][2:]])\n",
    "    if(row['Rotulo'][0] == 'I'):\n",
    "        if (entities): \n",
    "            entities[-1][1] = row['Fim']\n",
    "        else:\n",
    "            # print(index)\n",
    "            entities.append([row['Inicio'], row['Fim'], row['Rotulo'][2:]])\n",
    "    \n",
    "    if index == dataSet.index[-1]: # Ultimo elemento\n",
    "        if entities:\n",
    "            TRAIN_DATA.append((s, {\"entities\": [tuple(e) for e in entities]}))\n",
    "        entities = [] \n",
    "\n",
    "\n",
    "len(TRAIN_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_DATA = TRAIN_DATA[61:100]\n",
    "TRAIN_DATA = TRAIN_DATA[0:60]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAIN_DATA = TRAIN_DATA[0:1000]\n",
    "for item in TRAIN_DATA:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurando informações para o treinamento do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo carregado '<spacy.lang.pt.Portuguese object at 0x7f11ce008b80>'\n"
     ]
    }
   ],
   "source": [
    "# Carregando o modelo de nlp em português do spaCy. Esse modelo já foi baixado anteriormente\n",
    "\n",
    "nlp = spacy.load('pt_core_news_sm')\n",
    "print (\"Modelo carregado '% s'\"% nlp)\n",
    "# print(spacy.lang.char_classes.LIST_HYPHENS)\n",
    "# print(nlp.token_match)\n",
    "# print(nlp.Defaults.tokenizer_exceptions)\n",
    "# print(nlp.Defaults.infixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"\\\\'\", '\"', '”', '“', '`', '‘', '´', '’', '‚', ',', '„', '»', '«', '「', '」', '『', '』', '（', '）', '〔', '〕', '【', '】', '《', '》', '〈', '〉']\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.char_classes import LIST_PUNCT, LIST_ELLIPSES, LIST_QUOTES, LIST_CURRENCY, LIST_HYPHENS\n",
    "from spacy.lang.char_classes import LIST_ICONS, HYPHENS, CURRENCY\n",
    "from spacy.lang.char_classes import CONCAT_QUOTES, ALPHA_LOWER, ALPHA_UPPER, ALPHA\n",
    "# print(spacy.lang.char_classes.LIST_HYPHENS)\n",
    "print(LIST_QUOTES)\n",
    "# print(spacy.lang.char_classes.LIST_ELLIPSES)\n",
    "# print(spacy.lang.char_classes.LIST_ICONS)\n",
    "# print(spacy.lang.char_classes.ALPHA)\n",
    "# print(spacy.lang.char_classes.LIST_PUNCT)\n",
    "# print(spacy.lang.char_classes.ALPHA_LOWER)\n",
    "# print(spacy.lang.char_classes.ALPHA_UPPER)\n",
    "# print(spacy.lang.char_classes.CONCAT_QUOTES)\n",
    "\n",
    "# print(nlp.tokenizer.token_match)\n",
    "# print(list(nlp.Defaults.tokenizer_exceptions))\n",
    "# print(nlp.Defaults.infixes) \n",
    "# print(list(nlp.Defaults.suffixes))\n",
    "# print(nlp.Defaults.prefixes)\n",
    "# print(nlp.Defaults.stop_words)\n",
    "# print(nlp.tokenizer.rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.tokenizer.Tokenizer at 0x7f11ca95c040>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.symbols import ORTH\n",
    "def custom_tokenizer(nlp):\n",
    "    infixes = (\n",
    "        spacy.lang.char_classes.LIST_ELLIPSES\n",
    "        + spacy.lang.char_classes.LIST_ICONS\n",
    "        + [\"\\.\", \"\\/\", \":\", \"\\(\", \"\\)\"]\n",
    "        + [\n",
    "            r\"(?<=[0-9])[+\\-\\*^](?=[0-9-])\",\n",
    "            r\"(?<=[\\.])[\\.](?=[\\.])\",\n",
    "            r\"(?<=[{a}{q}])\\.(?=[{a}{q}])\".format(\n",
    "                a = spacy.lang.char_classes.ALPHA, q = spacy.lang.char_classes.CONCAT_QUOTES),\n",
    "            r\"(?<=[{a}]),(?=[{a}])\".format(a=spacy.lang.char_classes.ALPHA),\n",
    "            r\"(?<=[{a}])(?:{h}|\\/)(?=[{a}])\".format(a=spacy.lang.char_classes.ALPHA, h=spacy.lang.char_classes.LIST_HYPHENS),\n",
    "            r\"(?<=[{a}])(?:{h}|\\/)(?=[0-9])\".format(a=spacy.lang.char_classes.ALPHA, h=spacy.lang.char_classes.LIST_HYPHENS),\n",
    "            r\"(?<=[0-9])(?:{h}|\\/)(?=[{a}])\".format(a=spacy.lang.char_classes.ALPHA, h=spacy.lang.char_classes.LIST_HYPHENS),\n",
    "            r\"(?<=[0-9])(?:{h}|\\/|,|\\.)(?=[0-9])\".format(a=spacy.lang.char_classes.ALPHA, h=spacy.lang.char_classes.LIST_HYPHENS),\n",
    "            r\"(?<=[{a}0-9])[:<>=/](?=[{a}])\".format(a=spacy.lang.char_classes.ALPHA),\n",
    "            r\"(?<=[{a}\\\"]),(?=[0-9{a}])\".format(a=ALPHA),\n",
    "        ]\n",
    "    )\n",
    "    list_quotes = [\"\\\\'\", '\"', '`',\n",
    "                     '‘', '´', '’', '‚', ',',\n",
    "                     '„', '»', '«', '「', '」',\n",
    "                     '『', '』', '（', '）', '〔',\n",
    "                     '〕', '【', '】', '《', '》',\n",
    "                     '〈', '〉']\n",
    "    suffixes = (\n",
    "        LIST_PUNCT\n",
    "        + LIST_HYPHENS\n",
    "        + list_quotes\n",
    "        + LIST_ICONS\n",
    "        + [ \"\\/\", \":\", \"\\(\", \"\\)\"]\n",
    "        + [\"'s\", \"'S\", \"’s\", \"’S\", \"—\", \"–\"]\n",
    "        + [\n",
    "            r\"(?<=[0-9])\\+\",\n",
    "            # r\"(?<=°[FfCcKk])\\.\",\n",
    "            r\"(?<=[0-9])(?:{c})\".format(c=CURRENCY),\n",
    "            # r\"(?<=[0-9])(?:{u})\".format(u=UNITS),\n",
    "            r\"(?<=[0-9a-z(?:{q})])\\.\".format(\n",
    "            al=ALPHA_LOWER, q=CONCAT_QUOTES),\n",
    "            r\"(?<=[{au}])\\.\".format(au=ALPHA_UPPER),\n",
    "        ] \n",
    "    )\n",
    "    \n",
    "    prefixes = (\n",
    "        LIST_PUNCT\n",
    "        + [ \"\\/\", \":\", \"\\(\", \"\\)\"]\n",
    "        + LIST_HYPHENS\n",
    "        + list_quotes\n",
    "        + LIST_CURRENCY\n",
    "        + LIST_ICONS\n",
    "    )\n",
    "    # print(prefixes)\n",
    "\n",
    "    infix_re = spacy.util.compile_infix_regex(infixes)\n",
    "    suffix_re = spacy.util.compile_suffix_regex(suffixes)\n",
    "    prefix_re = spacy.util.compile_prefix_regex(prefixes)\n",
    "    # suffix_re = re.compile(r'''\\.[\\]\\)\"']$''')\n",
    "    custom = Tokenizer(nlp.vocab, prefix_search=prefix_re.search,\n",
    "                                suffix_search=suffix_re.search,\n",
    "                                infix_finditer=infix_re.finditer,\n",
    "                                token_match=nlp.tokenizer.token_match,\n",
    "                                rules=None)\n",
    "    special_case = [{ORTH: \".\"}, {ORTH: \".\"}, {ORTH: \".\"}]        # Adding special case rule\n",
    "    custom.add_special_case(\"...\", special_case)\n",
    "    return custom\n",
    "custom_tokenizer(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default tokenized text : Premiações, :, 1º, Lugar, na, ..., \", Exposição\",aa, 190, %, a-a, do, Formiga-MG, ”, ,, ), da, arte, a$, moderna/1922, 1º, Lugar, no, concurso, de, cachaça, 1111, 2003, de, qualidade, Hotel, Sofitel, Rio, de, Janeiro/2005, E, destaque, entre, as, 10, melhores, cachaças, envelhecida, na, Expocachaça, São, Paulo(Mercado, Municipal, ), 2011, ., \n",
      "AFTER tokenized text : Premiações \t TOKEN\n",
      ": \t PREFIX\n",
      "1º \t TOKEN\n",
      "Lugar \t TOKEN\n",
      "na \t TOKEN\n",
      ". \t SPECIAL-1\n",
      ". \t SPECIAL-2\n",
      ". \t SPECIAL-3\n",
      "\" \t PREFIX\n",
      "Exposição\" \t TOKEN\n",
      ", \t INFIX\n",
      "aa \t TOKEN\n",
      "190% \t TOKEN\n",
      "a \t TOKEN\n",
      "- \t INFIX\n",
      "a \t TOKEN\n",
      "do \t TOKEN\n",
      "Formiga \t TOKEN\n",
      "- \t INFIX\n",
      "MG” \t TOKEN\n",
      ", \t SUFFIX\n",
      ") \t SUFFIX\n",
      "da \t TOKEN\n",
      "arte \t TOKEN\n",
      "a$moderna \t TOKEN\n",
      "/ \t INFIX\n",
      "1922 \t TOKEN\n",
      "1º \t TOKEN\n",
      "Lugar \t TOKEN\n",
      "no \t TOKEN\n",
      "concurso \t TOKEN\n",
      "de \t TOKEN\n",
      "cachaça \t TOKEN\n",
      "1111 \t TOKEN\n",
      "2003 \t TOKEN\n",
      "de \t TOKEN\n",
      "qualidade \t TOKEN\n",
      "Hotel \t TOKEN\n",
      "Sofitel \t TOKEN\n",
      "Rio \t TOKEN\n",
      "de \t TOKEN\n",
      "Janeiro \t TOKEN\n",
      "/ \t INFIX\n",
      "2005 \t TOKEN\n",
      "E \t TOKEN\n",
      "destaque \t TOKEN\n",
      "entre \t TOKEN\n",
      "as \t TOKEN\n",
      "10 \t TOKEN\n",
      "melhores \t TOKEN\n",
      "cachaças \t TOKEN\n",
      "envelhecida \t TOKEN\n",
      "na \t TOKEN\n",
      "Expocachaça \t TOKEN\n",
      "São \t TOKEN\n",
      "Paulo \t TOKEN\n",
      "( \t INFIX\n",
      "Mercado \t TOKEN\n",
      "Municipal \t TOKEN\n",
      ") \t SUFFIX\n",
      "2011 \t TOKEN\n",
      ". \t SUFFIX\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Alterando Tokenizador\n",
    "from spacy.tokenizer import Tokenizer\n",
    "test_text = 'Premiações : 1º Lugar na ... \"Exposição\",aa 190% a-a do Formiga-MG”,) da arte a$moderna/1922 1º Lugar no concurso de cachaça 1111 2003 de qualidade Hotel Sofitel Rio de Janeiro/2005 E destaque entre as 10 melhores cachaças envelhecida na Expocachaça São Paulo(Mercado Municipal) 2011.'\n",
    "# test_text = \"ANÁLISE DO CACHACIER: De cor amarelo-clara, possui uma mescla de aromas florais e adocicados.\"\n",
    "# test_text = \"Premiações : 1º Lugar na Exposição do Centenário,semana da arte moderna/1922 1º Lugar no concurso de cachaça de qualidade Hotel Sofitel Rio de Janeiro/2005 E destaque entre as 10 melhores cachaças envelhecida na Expocachaça São Paulo(Mercado Municipal) 2011.\"\n",
    "# test_text = \"PREÇO: R$ 35,00\"\n",
    "doc = nlp(test_text)\n",
    "\n",
    "print(\"Default tokenized text\",end=' : ')\n",
    "for token in doc:\n",
    "    print(token,end=', ')\n",
    "\n",
    "\n",
    "\n",
    "nlp.tokenizer = custom_tokenizer(nlp)\n",
    "doc = nlp(test_text)\n",
    "\n",
    "print(\"\\nAFTER tokenized text\",end=' : ')\n",
    "tok_exp = nlp.tokenizer.explain(test_text)\n",
    "\n",
    "for t in tok_exp:\n",
    "    print(t[1], \"\\t\", t[0])\n",
    "\n",
    "\n",
    "# with open(\"/home/arthur/Documentos/IC/git/ProjetoMestrado/DataSets/Dados_Utilizados_Para_RotulacaoManual.txt\", \"r\", encoding='utf-8') as text_file:\n",
    "#     textos = text_file.read().splitlines()\n",
    "# print(\"Quantidade de linhas da base:\", len(textos))\n",
    "# for test_text in textos:\n",
    "#     doc = nlp(test_text)\n",
    "#     with open(\"tokensBySpacyFix.txt\",\"a\") as file:\n",
    "#         for token in doc:\n",
    "#                 print(token)\n",
    "#                 file.write(f\"{token}\\n\")\n",
    "\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print()\n",
    "# tok_exp = nlp.tokenizer.explain(test_text)\n",
    "# for t in tok_exp:\n",
    "#     print(t[1], \"\\t\", t[0])\n",
    "\n",
    "# print(list(nlp.Defaults.infixes))\n",
    "\n",
    "\n",
    "# with open(\"/home/arthur/Documentos/IC/git/ProjetoMestrado/DataSets/Dados_Utilizados_Para_RotulacaoManual.txt\", \"r\", encoding='utf-8') as text_file:\n",
    "#     textos = text_file.read().splitlines()\n",
    "# print(\"Quantidade de linhas da base:\", len(textos))\n",
    "# for test_text in textos:\n",
    "#     tok_exp = nlp.tokenizer.explain(test_text)\n",
    "# # for t in tok_exp:\n",
    "# #     print(t[1], \"\\t\", t[0])\n",
    "#     with open(\"tokensBySpacy.txt\",\"a\") as file:\n",
    "#         for t in tok_exp:\n",
    "#             print(t[1], \"\\t\", t[0])\n",
    "#             file.write(f\"{t[1]}\\t{t[0]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(nlp.Defaults.infixes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(infixes))\n",
    "# print(list(nlp.p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtendo o componente do pipeline para trabalhar com reconhecimentos de entidades nomeadas\n",
    "ner = nlp.get_pipe('ner')\n",
    "\n",
    "# Adicionando as categorias de entidade nomeada ao pipeline 'ner'\n",
    "ner.add_label('CARACTERISTICA_SENSORIAL_AROMA')\n",
    "ner.add_label('CARACTERISTICA_SENSORIAL_CONSISTÊNCIA')\n",
    "ner.add_label('CARACTERISTICA_SENSORIAL_SABOR')\n",
    "ner.add_label('CARACTERISTICA_SENSORIAL_COR')\n",
    "ner.add_label('RECIPIENTE_ARMAZENAMENTO')\n",
    "ner.add_label('EQUIPAMENTO_DESTILACAO')\n",
    "ner.add_label('CLASSIFICACAO_BEBIDA')\n",
    "ner.add_label('TEMPO_ARMAZENAMENTO')\n",
    "ner.add_label('GRADUACAO_ALCOOLICA')\n",
    "ner.add_label('TIPO_MADEIRA')\n",
    "ner.add_label('NOME_BEBIDA')\n",
    "ner.add_label('VOLUME')\n",
    "ner.add_label('NOME_LOCAL')\n",
    "ner.add_label('NOME_ORGANIZACAO')\n",
    "ner.add_label('NOME_PESSOA')\n",
    "ner.add_label('PRECO')\n",
    "ner.add_label('TEMPO')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# outra maneira mais rápida de adicionar as categorias de entidades ao 'ner'\n",
    "\n",
    "ner = nlp.get_pipe('ner')\n",
    "\n",
    "for texto, annotations in TRAIN_DATA:\n",
    "    for ent in annotations.get(\"entities\"):\n",
    "        ner.add_label(ent[2])     \n",
    "        print(ent[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorias de entidade que o modelo contém: \n",
      "\n",
      " {0: Counter(), 1: Counter({'LOC': 137478, 'PER': 65030, 'MISC': 38659, 'ORG': 25003, 'CARACTERISTICA_SENSORIAL_AROMA': -1, 'CARACTERISTICA_SENSORIAL_CONSISTÊNCIA': -2, 'CARACTERISTICA_SENSORIAL_SABOR': -3, 'CARACTERISTICA_SENSORIAL_COR': -4, 'RECIPIENTE_ARMAZENAMENTO': -5, 'EQUIPAMENTO_DESTILACAO': -6, 'CLASSIFICACAO_BEBIDA': -7, 'TEMPO_ARMAZENAMENTO': -8, 'GRADUACAO_ALCOOLICA': -9, 'TIPO_MADEIRA': -10, 'NOME_BEBIDA': -11, 'VOLUME': -12, 'NOME_LOCAL': -13, 'NOME_ORGANIZACAO': -14, 'NOME_PESSOA': -15, 'PRECO': -16, 'TEMPO': -17}), 2: Counter({'LOC': 137478, 'PER': 65030, 'MISC': 38659, 'ORG': 25003, 'CARACTERISTICA_SENSORIAL_AROMA': -1, 'CARACTERISTICA_SENSORIAL_CONSISTÊNCIA': -2, 'CARACTERISTICA_SENSORIAL_SABOR': -3, 'CARACTERISTICA_SENSORIAL_COR': -4, 'RECIPIENTE_ARMAZENAMENTO': -5, 'EQUIPAMENTO_DESTILACAO': -6, 'CLASSIFICACAO_BEBIDA': -7, 'TEMPO_ARMAZENAMENTO': -8, 'GRADUACAO_ALCOOLICA': -9, 'TIPO_MADEIRA': -10, 'NOME_BEBIDA': -11, 'VOLUME': -12, 'NOME_LOCAL': -13, 'NOME_ORGANIZACAO': -14, 'NOME_PESSOA': -15, 'PRECO': -16, 'TEMPO': -17}), 3: Counter({'LOC': 137478, 'PER': 65030, 'MISC': 38659, 'ORG': 25003, 'CARACTERISTICA_SENSORIAL_AROMA': -1, 'CARACTERISTICA_SENSORIAL_CONSISTÊNCIA': -2, 'CARACTERISTICA_SENSORIAL_SABOR': -3, 'CARACTERISTICA_SENSORIAL_COR': -4, 'RECIPIENTE_ARMAZENAMENTO': -5, 'EQUIPAMENTO_DESTILACAO': -6, 'CLASSIFICACAO_BEBIDA': -7, 'TEMPO_ARMAZENAMENTO': -8, 'GRADUACAO_ALCOOLICA': -9, 'TIPO_MADEIRA': -10, 'NOME_BEBIDA': -11, 'VOLUME': -12, 'NOME_LOCAL': -13, 'NOME_ORGANIZACAO': -14, 'NOME_PESSOA': -15, 'PRECO': -16, 'TEMPO': -17}), 4: Counter({'LOC': 137478, 'PER': 65030, 'MISC': 38659, 'ORG': 25003, '': 1, 'CARACTERISTICA_SENSORIAL_AROMA': -1, 'CARACTERISTICA_SENSORIAL_CONSISTÊNCIA': -2, 'CARACTERISTICA_SENSORIAL_SABOR': -3, 'CARACTERISTICA_SENSORIAL_COR': -4, 'RECIPIENTE_ARMAZENAMENTO': -5, 'EQUIPAMENTO_DESTILACAO': -6, 'CLASSIFICACAO_BEBIDA': -7, 'TEMPO_ARMAZENAMENTO': -8, 'GRADUACAO_ALCOOLICA': -9, 'TIPO_MADEIRA': -10, 'NOME_BEBIDA': -11, 'VOLUME': -12, 'NOME_LOCAL': -13, 'NOME_ORGANIZACAO': -14, 'NOME_PESSOA': -15, 'PRECO': -16, 'TEMPO': -17}), 5: Counter({'': 1})}\n"
     ]
    }
   ],
   "source": [
    "#verificando as categorias de entidades nomeadas contidas no pipeline\n",
    "print(\"Categorias de entidade que o modelo contém:\", '\\n\\n', ner.label_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obter os nomes dos componentes que queremos desativar durante o treinamento\n",
    "\n",
    "#pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "#unaffected_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "unaffected_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinando o modelo de reconhecimento de entidades nomeadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-21 20:55:56.431104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arthur/.local/lib/python3.8/site-packages/spacy/training/iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"PREMIOS: 2017 - 2º Lugar na 27ª Edição Expocachaça...\" with entities \"[(9, 13, 'TEMPO'), (53, 67, 'NOME_LOCAL'), (68, 70...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses (1/1) - 2022-03-21 21:11:45.572053 {'ner': 52821.746543047004}\n"
     ]
    }
   ],
   "source": [
    "# iniciar o ciclo de treinamento, apenas treinando NER\n",
    "\n",
    "epochs = 1\n",
    "optimizer = nlp.create_optimizer()\n",
    "#optimizer = nlp.resume_training()\n",
    "#optimizer = nlp.begin_training()\n",
    "\n",
    "with nlp.disable_pipes(*unaffected_pipes), warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"once\", category=UserWarning, module='spacy')\n",
    "    print(date.datetime.now())\n",
    "    sizes = compounding(4.0, 32.0, 1.001)\n",
    "    \n",
    "    # agrupe os exemplos usando o minibatc do spaCy\n",
    "    for epoch in range(epochs):\n",
    "        examples = TRAIN_DATA\n",
    "        random.shuffle(examples)\n",
    "        batches = minibatch(examples, size=sizes)\n",
    "        losses = {}\n",
    "        \n",
    "        for batch in batches:\n",
    "            texts, annotations = zip(*batch)\n",
    "            example = []\n",
    "            # Atualize o modelo com a iteração de cada texto\n",
    "            for i in range(len(texts)):\n",
    "                doc = nlp.make_doc(texts[i])\n",
    "                example.append(Example.from_dict(doc, annotations[i]))\n",
    "                # Update the model\n",
    "                nlp.update(example, drop=0.35, losses=losses,sgd=optimizer)\n",
    "        print(\"Losses ({}/{})\" .format(epoch + 1, epochs), '-', date.datetime.now(), losses)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testando o Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo using o display sentence involving original entities\n",
    "spacy.displacy.render(nlp(\"A Bacuri é uma cachaça, tem um sabor tropical que vai aumentar o seu ânimo e envolvê-lo em uma onda de puro prazer. É armazenada em barris de jequitibá\"), style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregando alguns Dados de Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados_Teste = [('A Cachaça Serra Limpa Prata 355ml é uma bebida armazenada por seis meses em tonéis de inox. Robusta. É uma das únicas Cachaças Artesanais cem porcento orgânicas do Brasil. Contém 50% de graduação alcoolia',\n",
    "  {'entities': [(76, 91, 'RECIPIENTE_ARMAZENAMENTO')]}),\n",
    "  \n",
    "  ('A Cachaça Minha Deusa Vale Verde Prata 1000ml é uma bebida armazenada por um ano em tonéis de grápia, o que realça o aroma de cana-de-açúcar. Leve,. De sabor suave e marcante, é perfeita para o preparo de drinks e coquetéis.',\n",
    "  {'entities': [(84, 100, 'RECIPIENTE_ARMAZENAMENTO')]}),\n",
    "\n",
    "  ('Cachaça Mineirinha 355ml é uma edição especial das Cachaças Dona Beja, de Perdizes, Minas Gerais. Armazenada por cinco anos em barris de jequitiba. Chama a atenção a embalagem da bebida, que é envasada em garrafas de louça que imitam a cana-de-açúcar.',\n",
    "  {'entities': [(127, 147, 'RECIPIENTE_ARMAZENAMENTO')]}),\n",
    " \n",
    "  ('Três Corações é uma bebida armazenada por 2 anos em jequitibá. Robusta e com aroma doce', \n",
    "  {'entities':[(52, 61, 'RECIPIENTE_ARMAZENAMENTO')]})          \n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avaliação do Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compara item por item o que foi classificado pelo modelo e a classifcação correta(manual)\n",
    "\n",
    "#nlp = spacy.load(output_dir)\n",
    "examples = []\n",
    "\n",
    "for text, annots in dados_Teste:\n",
    "    print(f'Manual Anotation: {annots}')\n",
    "    doc = nlp(text)\n",
    "    #doc = nlp.make_doc(text)\n",
    "    spacy.displacy.render(nlp(text))\n",
    "    \n",
    "    for item in doc.ents:\n",
    "        print('Model prediction: ', 'entities: ', [(item.start_char, item.end_char,item.label_)])\n",
    "                \n",
    "    examples.append(Example.from_dict(doc, annots)) \n",
    "    #print('Tokens', [(t.text, t.ent_type_, t.ent_iob) for t in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Métricas Precision, Recall e f-measure: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apresenta o resultados das métricas para o modelo como um todo, e as métricas do modelo para cada categoria de entidade\n",
    "\n",
    "print(nlp.evaluate(examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avaliação do Modelo pelas Métrica Acuracia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary which will be populated with the entities and result information\n",
    "entity_evaluation = {}\n",
    "\n",
    "# helper function to udpate the entity_evaluation dictionary\n",
    "def update_results(entity, metric):\n",
    "    if entity not in entity_evaluation:\n",
    "        entity_evaluation[entity] = {\"correct\": 0, \"total\": 0}\n",
    "    \n",
    "    entity_evaluation[entity][metric] += 1\n",
    "\n",
    "# same as before, see if entities from test set match what spaCy currently predicts\n",
    "for data in dados_Teste:\n",
    "    sentence = data[0]\n",
    "    entities = data[1][\"entities\"]\n",
    "\n",
    "    for entity in entities:\n",
    "        doc = nlp(sentence)\n",
    "        correct_text = sentence[entity[0]:entity[1]]\n",
    "\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ == entity[2] and ent.text == correct_text:\n",
    "                update_results(ent.label_, \"correct\")\n",
    "                break\n",
    "\n",
    "        update_results(entity[2], \"total\")\n",
    "print(\"Resultado para cada categoria:\",'\\n', entity_evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_total = 0\n",
    "sum_correct = 0\n",
    "\n",
    "for entity in entity_evaluation:\n",
    "    total = entity_evaluation[entity][\"total\"]\n",
    "    correct = entity_evaluation[entity][\"correct\"]\n",
    "\n",
    "    sum_total += total\n",
    "    sum_correct += correct\n",
    "    \n",
    "    print(\"{} | {:.2f}%\".format(entity, correct / total * 100))\n",
    "\n",
    "print()\n",
    "print(\"Overall accuracy: {:.2f}%\".format(sum_correct / sum_total * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salvando o Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the  model to directory\n",
    "\n",
    "from pathlib import Path\n",
    "output_dir=Path('C:\\\\Users\\\\prisc\\\\Downloads\\\\Modelo_spaCy\\\\model_saved')\n",
    "nlp.to_disk(output_dir)\n",
    "print(\"Saved model to\", output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importando o Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model and predict\n",
    "\n",
    "print(\"Loading from\", output_dir)\n",
    "nlp_updated = spacy.load(output_dir)\n",
    "doc = nlp_updated(\"Eu gostei da Cachaça Artesanal criada em Minas Gerais\" )\n",
    "print(\"Entidades\", [(ent.text, ent.label_) for ent in doc.ents])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

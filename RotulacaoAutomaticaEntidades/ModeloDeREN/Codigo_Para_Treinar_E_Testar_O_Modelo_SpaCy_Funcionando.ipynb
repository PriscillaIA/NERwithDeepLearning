{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "import glob\n",
    "import csv\n",
    "from spacy.training import offsets_to_biluo_tags\n",
    "from spacy.training import biluo_tags_to_spans\n",
    "from spacy.training import offsets_to_biluo_tags, biluo_tags_to_offsets, biluo_tags_to_spans\n",
    "import itertools\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# importações para o bloco de código do treinamento do modelo de NER\n",
    "import random\n",
    "from spacy.util import minibatch, compounding\n",
    "from pathlib import Path\n",
    "from spacy.tokens import Doc\n",
    "from spacy.training import Example\n",
    "import warnings\n",
    "import datetime as date\n",
    "from datetime import datetime\n",
    "import time\n",
    "from spacy.scorer import Scorer\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregando os Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5855"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Dados de treinamento rotulado pelo algorimto de rotulação automatica\n",
    "with open(\"./DadosResultantesDaRotulacaoAutomatica/Resultado_com_100_porcento.bin\", \"rb\") as arquivo:\n",
    "    dataSetEntradaTreinamento = pickle.load(arquivo)\n",
    "len(dataSetEntradaTreinamento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do dataset de teste 53639\n"
     ]
    }
   ],
   "source": [
    "#Carrega o dataset IOB, ou seja, os tokens são rotulados com tags do tipo marcação IOB.\n",
    "dataset_teste = pd.read_csv(\"dataset_rotulado_teste.csv\", usecols=['Palavras', 'Rotulo', 'Sentenca', 'Inicio', 'Fim','Documento'])\n",
    "print(\"Tamanho do dataset de teste\",len(dataset_teste))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantida de de sentenças no dataSet utilizado na rotulação manual: 13628\n"
     ]
    }
   ],
   "source": [
    "#Carrega o dataset utilizada anteriormente na rotulação manual das categorias de entidades nomeadas, sem os IDs que identificam os documentos\n",
    "#Esse arquivo é utilizado para extrairmos as setenças a que cada token rotulado no dataset pertence. \n",
    "#Essa informação é utilizada no código de conversão do dados no formato de marcação IOB para o formato compreendido pelo spaCy.\n",
    "with open(\"dataset_para_rotulacao_manual_versao_2.txt\", \"r\", encoding='utf-8') as file:\n",
    "    textos = file.read().splitlines()\n",
    "print(\"Quantida de de sentenças no dataSet utilizado na rotulação manual:\", len(textos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convertendo os dados do formato IOB para o formato reconhecido pelo spaCy\n",
    "Dataset de treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de sentenças do dataset de teste: 4174\n"
     ]
    }
   ],
   "source": [
    "sMarker = 0 #Marcador que referência a qual setença a entidade pertence, ou seja, serve pra pegar o texto da referencia dos tokens no dataset\n",
    "s = textos[sMarker]\n",
    "dataSetEntradaTeste = []\n",
    "entities = []\n",
    "for index, row in dataset_teste.iterrows():\n",
    "    \n",
    "    if(row['Sentenca']-1 != sMarker): # Nova sentença\n",
    "        if entities:\n",
    "            dataSetEntradaTeste.append((s, {\"entities\": [tuple(e) for e in entities]})) # Salva sentença anterior\n",
    "        entities = [] # esvazia entidades\n",
    "        sMarker = row['Sentenca']-1 # atualiza o marcador de sentença\n",
    "        \n",
    "        if (sMarker < len(textos)): # Limite de textos\n",
    "            s = textos[sMarker]\n",
    "            \n",
    "    if(row['Rotulo'][0] == 'O'):\n",
    "         entities.append([row['Inicio'], row['Fim'], row['Rotulo'][2:]])\n",
    "            \n",
    "    if(row['Rotulo'][0] == 'B'):\n",
    "         entities.append([row['Inicio'], row['Fim'], row['Rotulo'][2:]])\n",
    "            \n",
    "    if(row['Rotulo'][0] == 'I'):\n",
    "        if (entities): \n",
    "            entities[-1][1] = row['Fim']\n",
    "        else:\n",
    "            # print(index)\n",
    "            entities.append([row['Inicio'], row['Fim'], row['Rotulo'][2:]])\n",
    "            \n",
    "    if index == dataset_teste.index[-1]: # Ultimo elemento\n",
    "        if entities:\n",
    "            dataSetEntradaTeste.append((s, {\"entities\": [tuple(e) for e in entities]}))\n",
    "        entities = [] \n",
    "\n",
    "print('Quantidade de sentenças do dataset de teste:', len(dataSetEntradaTeste))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importando modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo carregado '<spacy.lang.pt.Portuguese object at 0x000002035D1BEA90>'\n"
     ]
    }
   ],
   "source": [
    "# Carregando um modelo do spaCy em portguês previamente treinando\n",
    "nlp = spacy.load('pt_core_news_sm')\n",
    "print (\"Modelo carregado '% s'\"% nlp)\n",
    "\n",
    "##carregando um modelo em braco\n",
    "#nlp = spacy.blank('pt')\n",
    "#nlp.vocab.vectors.name = 'spacy_pretrained_vectors'\n",
    "#print (\"Modelo carregado '% s'\"% nlp)\n",
    "#ner = nlp.create_pipe('ner')\n",
    "#nlp.add_pipe('ner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customizando o Tokenizador para o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.symbols import ORTH\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.char_classes import LIST_PUNCT, LIST_ELLIPSES, LIST_QUOTES, LIST_CURRENCY, LIST_HYPHENS\n",
    "from spacy.lang.char_classes import LIST_ICONS, HYPHENS, CURRENCY\n",
    "from spacy.lang.char_classes import CONCAT_QUOTES, ALPHA_LOWER, ALPHA_UPPER, ALPHA\n",
    "\n",
    "# Essa função cria um objeto customizado da classe Tokenizer do spacy, para alinhamento dos tokens com a rotulações em alguns casos especiais por exemplo: '-' e '$'.\n",
    "def custom_tokenizer(nlp):    \n",
    "    infixes = (\n",
    "        spacy.lang.char_classes.LIST_ELLIPSES\n",
    "        + spacy.lang.char_classes.LIST_ICONS\n",
    "        + [\"\\.\", \"\\/\", \":\", \"\\(\", \"\\)\"]\n",
    "        + [\n",
    "            r\"(?<=[0-9])[+\\-\\*^](?=[0-9-])\",\n",
    "            r\"(?<=[\\.])[\\.](?=[\\.])\",\n",
    "            r\"(?<=[{a}{q}])\\.(?=[{a}{q}])\".format(\n",
    "                a = ALPHA, q = CONCAT_QUOTES),\n",
    "            r\"(?<=[{a}]),(?=[{a}])\".format(a=ALPHA),\n",
    "            r\"(?<=[{a}])(?:{h}|\\/)(?=[{a}])\".format(a=ALPHA, h=LIST_HYPHENS),\n",
    "            r\"(?<=[{a}])(?:{h}|\\/)(?=[0-9])\".format(a=ALPHA, h=LIST_HYPHENS),\n",
    "            r\"(?<=[0-9])(?:{h}|\\/)(?=[{a}])\".format(a=ALPHA, h=LIST_HYPHENS),\n",
    "            r\"(?<=[0-9])(?:{h}|\\/|,|\\.)(?=[0-9])\".format(a=ALPHA, h=LIST_HYPHENS),\n",
    "            r\"(?<=[{a}0-9])[:<>=/](?=[{a}])\".format(a=ALPHA),\n",
    "            r\"(?<=[{a}\\\"]),(?=[0-9{a}])\".format(a=ALPHA),\n",
    "        ]\n",
    "    )\n",
    "    list_quotes = [\"\\\\'\", '\"', '`',\n",
    "                     '‘', '´', '’', '‚', ',',\n",
    "                     '„', '»', '«', '「', '」',\n",
    "                     '『', '』', '（', '）', '〔',\n",
    "                     '〕', '【', '】', '《', '》',\n",
    "                     '〈', '〉']\n",
    "    suffixes = (\n",
    "        LIST_PUNCT\n",
    "        + LIST_HYPHENS\n",
    "        + list_quotes\n",
    "        + LIST_ICONS\n",
    "        + [ \"\\/\", \":\", \"\\(\", \"\\)\"]\n",
    "        + [\"'s\", \"'S\", \"’s\", \"’S\", \"—\", \"–\"]\n",
    "        + [\n",
    "            r\"(?<=[0-9])\\+\",\n",
    "            r\"(?<=[0-9])(?:{c})\".format(c=CURRENCY),\n",
    "            r\"(?<=[0-9a-z(?:{q})])\\.\".format(\n",
    "            al=ALPHA_LOWER, q=CONCAT_QUOTES),\n",
    "            r\"(?<=[{au}])\\.\".format(au=ALPHA_UPPER),\n",
    "        ] \n",
    "    )\n",
    "    \n",
    "    prefixes = (\n",
    "        LIST_PUNCT\n",
    "        + [ \"\\/\", \":\", \"\\(\", \"\\)\"]\n",
    "        + LIST_HYPHENS\n",
    "        + list_quotes\n",
    "        + LIST_CURRENCY\n",
    "        + LIST_ICONS\n",
    "    )\n",
    "\n",
    "    infix_re = spacy.util.compile_infix_regex(infixes)\n",
    "    suffix_re = spacy.util.compile_suffix_regex(suffixes)\n",
    "    prefix_re = spacy.util.compile_prefix_regex(prefixes)\n",
    "    custom = Tokenizer(nlp.vocab, prefix_search=prefix_re.search,\n",
    "                                suffix_search=suffix_re.search,\n",
    "                                infix_finditer=infix_re.finditer,\n",
    "                                token_match=nlp.tokenizer.token_match,\n",
    "                                rules=None)\n",
    "    special_case = [{ORTH: \".\"}, {ORTH: \".\"}, {ORTH: \".\"}]        # Adicionando uma regra especial\n",
    "    custom.add_special_case(\"...\", special_case)\n",
    "    return custom\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# atribuindo o tokenizador customizado ao modelo nlp\n",
    "nlp.tokenizer = custom_tokenizer(nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurando parâmetros e dados para o treinamento do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtendo o componente do pipeline (ner) para trabalhar com reconhecimentos de entidades nomeadas\n",
    "ner = nlp.get_pipe('ner')\n",
    "# Adicionando as categorias de entidade nomeada ao pipeline 'ner' manualmente\n",
    "ner.add_label('CARACTERISTICA_SENSORIAL_AROMA')\n",
    "ner.add_label('CARACTERISTICA_SENSORIAL_CONSISTÊNCIA')\n",
    "ner.add_label('CARACTERISTICA_SENSORIAL_SABOR')\n",
    "ner.add_label('CARACTERISTICA_SENSORIAL_COR')\n",
    "ner.add_label('RECIPIENTE_ARMAZENAMENTO')\n",
    "ner.add_label('EQUIPAMENTO_DESTILACAO')\n",
    "ner.add_label('CLASSIFICACAO_BEBIDA')\n",
    "ner.add_label('TEMPO_ARMAZENAMENTO')\n",
    "ner.add_label('GRADUACAO_ALCOOLICA')\n",
    "ner.add_label('TIPO_MADEIRA')\n",
    "ner.add_label('NOME_BEBIDA')\n",
    "ner.add_label('VOLUME')\n",
    "ner.add_label('NOME_LOCAL')\n",
    "ner.add_label('NOME_ORGANIZACAO')\n",
    "ner.add_label('NOME_PESSOA')\n",
    "ner.add_label('PRECO')\n",
    "ner.add_label('TEMPO')\n",
    "\n",
    "#print(\"Categorias de entidade que o modelo contém:\", '\\n\\n', ner.label_data)\n",
    "# outra maneira mais rápida de adicionar as categorias de entidades ao 'ner'\n",
    "#ner = nlp.get_pipe('ner')\n",
    "#for texto, annotations in TRAIN_DATA:\n",
    "    #for ent in annotations.get(\"entities\"):\n",
    "        #ner.add_label(ent[2])     \n",
    "        #print(ent[2])\n",
    "#print(\"Categorias de entidade que o modelo contém:\", '\\n\\n', ner.label_data) #verificando as categorias de entidades nomeadas contidas no pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtendo os nomes dos componentes que queremos desativar durante o treinamento\n",
    "unaffected_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "#Abaixo temos outra maneira de fazer a mesma cois, mas com duas linhas de comando\n",
    "#pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "#unaffected_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tranforma os dados do conjunto de treino no formato entendido pelo modelo ner\n",
    "examples_train = []\n",
    "for text, annots in dataSetEntradaTreinamento:\n",
    "    examples_train.append(Example.from_dict(nlp.make_doc(text), annots))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinando o modelo de reconhecimento de entidades nomeadas\n",
    "O código abaixo realiza o treinamento do modelo de reconhecimento de entidades nomeadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perda acumulada do modelo ({'ner': 112433.80672789947}) \n",
      "\n",
      "Tempo gasto para treinar o modelo em segundos --> 1895.9201459884644 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#VTreinamento do modelo ner com novas categorias de entidades nomeadas\n",
    "import decimal\n",
    "optimizar = nlp.create_optimizer() #quando utilizamos o modelo já treinado do spaCy\n",
    "#optimizar = nlp.begin_training() #quando é criado um modelo nlp vazio\n",
    "epochsSize = 50\n",
    "batchSize = 64\n",
    "drop_size = 0.30\n",
    "losses = {}\n",
    "with nlp.disable_pipes(*unaffected_pipes), warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"once\", category=UserWarning, module='spacy') \n",
    "    start_epoch = time.time()\n",
    "    for epoch in range(epochsSize):\n",
    "        random.shuffle(examples_train)\n",
    "        #count = 0\n",
    "        for batch in minibatch(examples_train, size=batchSize):\n",
    "            nlp.update(batch, losses=losses, drop=drop_size, sgd=optimizar) \n",
    "            and_epoch = time.time()            \n",
    "            #count+=1\n",
    "            #valorPerda = losses['ner'] # valorPerda = decimal.Decimal(losses['ner'])\n",
    "            #print(\"Perda acumulada ({}) para o batch ({})\" .format(valorPerda,count,\"\\n\"))\n",
    "            tempoParaTreinarModelo = and_epoch-start_epoch \n",
    "            #metricas = nlp.evaluate(examples_validation)\n",
    "            #print('Métricas de validação do modelo para o batch {}'.format(count),'\\n')\n",
    "            #print(metricas,'\\n')\n",
    "\n",
    "        #print(\"Perda acumulada ({}) para a epóca ({} de {})\" .format(losses, epoch + 1, epochsSize),'\\n') \n",
    "    print(\"Perda acumulada do modelo ({})\" .format(losses),\"\\n\")\n",
    "    print('Tempo gasto para treinar o modelo em segundos -->', tempoParaTreinarModelo,'\\n')\n",
    "    #metricas=nlp.evaluate(examples_validation)\n",
    "    #print('Métricas Final de validação do modelo com {} épocas e {} batchs'.format(epochsSize),'\\n') \n",
    "    #print(metricas,'\\n') \n",
    "    #scorer = Scorer()\n",
    "    #result=scorer.score(examples)\n",
    "    #print('SCORER POR BATC:', result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparação Rotulação Manual versus Rotulação do Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tranforma os dados do conjunto de teste no formato entendido pelo modelo ner\n",
    "examples_test = []\n",
    "for text, annots in dataSetEntradaTeste:\n",
    "    examples_test.append(Example.from_dict(nlp.make_doc(text), annots))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import collections\n",
    "# Compara item por item como o modelo classificou as sentenças e como elas foram classificadas manual (ponto de referencia).\n",
    "#nlp = spacy.load(output_dir)\n",
    "examples = []\n",
    "comparacao= []\n",
    "lista_rotulacaomanual = []\n",
    "lista_rotulacaoautomatica = []\n",
    "count_1 = 0\n",
    "count_2 = 0\n",
    "for text, annots in dataSetEntradaTeste:\n",
    "    count_1 =count_1+1\n",
    "    count_2 =count_2+1\n",
    "    doc = nlp(text)\n",
    "    anotacao_manual = annots['entities']    \n",
    "    result_model = [(item.start_char, item.end_char,item.label_) for item in doc.ents]\n",
    "    #print(f'Classificação Manual:', anotacao_manual,'\\n')\n",
    "    #print(f'Classificação do Modelo:', result_model,'\\n')\n",
    "    \n",
    "  \n",
    "    tokens_rotulados_rotulacaomanual = [item for item in anotacao_manual if item[2] != '']\n",
    "    tokens_rotulados_rotulacaoautomatica = [item for item in result_model if item[2] != '']\n",
    "    \n",
    "    if len(tokens_rotulados_rotulacaomanual) < 1:\n",
    "        tokens_rotulados_rotulacaomanual = [(\"-\",\"Sem entidade\",\"-\")]       \n",
    "    if len(tokens_rotulados_rotulacaoautomatica) < 1:\n",
    "        tokens_rotulados_rotulacaoautomatica = [(\"-\",\"Sem entidade\",\"-\")]  \n",
    "    \n",
    "    #print(\"MANUAL\")\n",
    "    #print(tokens_rotulados_rotulacaomanual, count)\n",
    "    #print(\"AUTOMATICO\")\n",
    "    #print(tokens_rotulados_rotulacaoautomatica, count)\n",
    "    \n",
    "    for i in tokens_rotulados_rotulacaomanual:\n",
    "        #print(i,\";\", count_1)  \n",
    "        lista_rotulacaomanual.append((i,\";\", count_1))\n",
    "    for j in tokens_rotulados_rotulacaoautomatica:\n",
    "        #print(j,\";\",count_2)  \n",
    "        lista_rotulacaoautomatica.append((j,\";\", count_2))\n",
    "#verifica_listas = bool(set(result_model) & set(tokens_rotulados_rotulacaomanual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8104"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista_rotulacaomanual_1=[]\n",
    "for item in lista_rotulacaomanual:\n",
    "    lista_rotulacaomanual_1.append(item)\n",
    "len(lista_rotulacaomanual_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8929"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista_rotulacaomanual_2=[]\n",
    "for item in lista_rotulacaoautomatica:\n",
    "    lista_rotulacaomanual_2.append(item)\n",
    "len(lista_rotulacaomanual_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with open(\"rotulacaomanual.txt\", \"w\",encoding='utf-8') as arquivo:\n",
    "for item in lista_rotulacaomanual:\n",
    "    arquivo.write(\"%s\\n\" % str(item))\n",
    "print('Done')\n",
    "with open(\"rotulacaoautomatica.txt\", \"w\",encoding='utf-8') as arquivo:\n",
    "for item in lista_rotulacaoautomatica:\n",
    "    arquivo.write(\"%s\\n\" % str(item))\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Métricas Precision, Recall e f-measure: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'token_acc': 1.0, 'token_p': 1.0, 'token_r': 1.0, 'token_f': 1.0, 'pos_acc': None, 'morph_acc': None, 'morph_per_feat': None, 'sents_p': 1.0, 'sents_r': 1.0, 'sents_f': 1.0, 'dep_uas': None, 'dep_las': None, 'dep_las_per_type': None, 'ents_p': 0.6717351318209688, 'ents_r': 0.7411716953051009, 'ents_f': 0.7047472018525666, 'ents_per_type': {'NOME_BEBIDA': {'p': 0.6666666666666666, 'r': 0.7391304347826086, 'f': 0.7010309278350515}, 'PRECO': {'p': 0.8629629629629629, 'r': 0.8661710037174721, 'f': 0.8645640074211504}, 'VOLUME': {'p': 0.8153034300791556, 'r': 0.807843137254902, 'f': 0.8115561391989493}, 'GRADUACAO_ALCOOLICA': {'p': 0.2777777777777778, 'r': 0.013966480446927373, 'f': 0.02659574468085106}, 'RECIPIENTE_ARMAZENAMENTO': {'p': 0.7196765498652291, 'r': 0.89, 'f': 0.7958271236959761}, 'TIPO_MADEIRA': {'p': 0.7749712973593571, 'r': 0.8302583025830258, 'f': 0.8016627078384799}, 'NOME_LOCAL': {'p': 0.8615622583139985, 'r': 0.8764752163650669, 'f': 0.8689547581903277}, 'NOME_ORGANIZACAO': {'p': 0.6779026217228464, 'r': 0.565625, 'f': 0.616695059625213}, 'TEMPO_ARMAZENAMENTO': {'p': 0.5642201834862385, 'r': 0.634020618556701, 'f': 0.5970873786407768}, 'CARACTERISTICA_SENSORIAL_SABOR': {'p': 0.24428822495606328, 'r': 0.5018050541516246, 'f': 0.3286052009456265}, 'CARACTERISTICA_SENSORIAL_COR': {'p': 0.5441176470588235, 'r': 0.74, 'f': 0.6271186440677965}, 'TEMPO': {'p': 0.9607843137254902, 'r': 0.8386308068459658, 'f': 0.8955613577023499}, 'CARACTERISTICA_SENSORIAL_CONSISTÊNCIA': {'p': 0.7373737373737373, 'r': 0.8588235294117647, 'f': 0.7934782608695652}, 'CLASSIFICACAO_BEBIDA': {'p': 0.5361890694239291, 'r': 0.8402777777777778, 'f': 0.654643823264202}, 'CARACTERISTICA_SENSORIAL_AROMA': {'p': 0.35578947368421054, 'r': 0.6014234875444839, 'f': 0.4470899470899471}, 'NOME_PESSOA': {'p': 0.6910569105691057, 'r': 0.7657657657657657, 'f': 0.7264957264957265}, 'EQUIPAMENTO_DESTILACAO': {'p': 0.32947976878612717, 'r': 0.6705882352941176, 'f': 0.4418604651162791}}, 'tag_acc': None, 'lemma_acc': None, 'speed': 4843.786640511923}\n"
     ]
    }
   ],
   "source": [
    "# Apresenta o resultados das métricas para o modelo como um todo, e as métricas do modelo para cada categoria de entidade\n",
    "calculatedBySpacy = nlp.evaluate(examples_test)\n",
    "print(calculatedBySpacy)\n",
    "\n",
    "#from spacy.scorer import Scorer\n",
    "#scorer = Scorer()   \n",
    "#example_scores = []\n",
    "#for text, annot in dataSetEntradaTeste:\n",
    "#    print(text)\n",
    "#    pred = nlp.make_doc(text)\n",
    "#    temp = Example.from_dict(pred, annot)\n",
    "#    example_scores.append(temp)\n",
    "#    scores = scorer.score(example_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avaliação do Modelo pela Métrica Acurácia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados corretamente classificados versus total de dados corretos: \n",
      " {'': {'correct': 0, 'total': 41711}, 'NOME_BEBIDA': {'correct': 715, 'total': 966}, 'PRECO': {'correct': 233, 'total': 269}, 'VOLUME': {'correct': 618, 'total': 765}, 'GRADUACAO_ALCOOLICA': {'correct': 5, 'total': 358}, 'TIPO_MADEIRA': {'correct': 675, 'total': 813}, 'NOME_ORGANIZACAO': {'correct': 181, 'total': 320}, 'NOME_LOCAL': {'correct': 1115, 'total': 1271}, 'TEMPO_ARMAZENAMENTO': {'correct': 246, 'total': 388}, 'RECIPIENTE_ARMAZENAMENTO': {'correct': 267, 'total': 300}, 'CARACTERISTICA_SENSORIAL_SABOR': {'correct': 139, 'total': 277}, 'CARACTERISTICA_SENSORIAL_COR': {'correct': 111, 'total': 150}, 'TEMPO': {'correct': 343, 'total': 409}, 'CLASSIFICACAO_BEBIDA': {'correct': 363, 'total': 432}, 'CARACTERISTICA_SENSORIAL_AROMA': {'correct': 169, 'total': 281}, 'CARACTERISTICA_SENSORIAL_CONSISTÊNCIA': {'correct': 73, 'total': 85}, 'NOME_PESSOA': {'correct': 170, 'total': 222}, 'EQUIPAMENTO_DESTILACAO': {'correct': 57, 'total': 85}}\n"
     ]
    }
   ],
   "source": [
    "# dictionary which will be populated with the entities and result information\n",
    "entity_evaluation = {}\n",
    "# helper function to udpate the entity_evaluation dictionary\n",
    "def update_results(entity, metric):\n",
    "    if entity not in entity_evaluation:\n",
    "        entity_evaluation[entity] = {\"correct\": 0, \"total\": 0}  \n",
    "    entity_evaluation[entity][metric] += 1\n",
    "    \n",
    "# same as before, see if entities from test set match what spaCy currently predicts\n",
    "example_scores = []\n",
    "for data in dataSetEntradaTeste:\n",
    "    sentence = data[0]\n",
    "    entities = data[1][\"entities\"]\n",
    "    for entity in entities:\n",
    "        doc = nlp(sentence)\n",
    "        correct_text = sentence[entity[0]:entity[1]]\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ == entity[2] and ent.text == correct_text:\n",
    "                update_results(ent.label_, \"correct\")\n",
    "                break\n",
    "        update_results(entity[2], \"total\")\n",
    "\n",
    "print(\"Dados corretamente classificados versus total de dados corretos:\",'\\n', entity_evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculo Acuracia por Categoria e Acuracia Total\n",
      "NOME_BEBIDA --> 0.74%\n",
      "PRECO --> 0.87%\n",
      "VOLUME --> 0.81%\n",
      "GRADUACAO_ALCOOLICA --> 0.01%\n",
      "TIPO_MADEIRA --> 0.83%\n",
      "NOME_ORGANIZACAO --> 0.57%\n",
      "NOME_LOCAL --> 0.88%\n",
      "TEMPO_ARMAZENAMENTO --> 0.63%\n",
      "RECIPIENTE_ARMAZENAMENTO --> 0.89%\n",
      "CARACTERISTICA_SENSORIAL_SABOR --> 0.50%\n",
      "CARACTERISTICA_SENSORIAL_COR --> 0.74%\n",
      "TEMPO --> 0.84%\n",
      "CLASSIFICACAO_BEBIDA --> 0.84%\n",
      "CARACTERISTICA_SENSORIAL_AROMA --> 0.60%\n",
      "CARACTERISTICA_SENSORIAL_CONSISTÊNCIA --> 0.86%\n",
      "NOME_PESSOA --> 0.77%\n",
      "EQUIPAMENTO_DESTILACAO --> 0.67%\n",
      "Acuracia Total (todas as categorias): 0.74%\n",
      "Total de entidades Rotulados Manualmente (referencia): 7391\n",
      "Total de entidades Rotuladas Corretamente pelo modelo: 5480\n",
      "Total de entidades Rotuladas Incorretamente pelo modelo: 1911\n"
     ]
    }
   ],
   "source": [
    "print(\"Calculo Acuracia por Categoria e Acuracia Total\")\n",
    "sum_total = 0\n",
    "sum_correct = 0\n",
    "sum_incorretos_total = 0\n",
    "accuracy_by_category = {}\n",
    "\n",
    "for entity in entity_evaluation:\n",
    "    if entity != '':\n",
    "        total = entity_evaluation[entity][\"total\"]\n",
    "        correct = entity_evaluation[entity][\"correct\"]\n",
    "        rotulados_incorretamente = total-correct\n",
    "        sum_total += total\n",
    "        sum_correct += correct\n",
    "        sum_incorretos_total += rotulados_incorretamente\n",
    "        percentual_acertos_modelo_por_categoria = correct/total\n",
    "        #total_acerto_por_categoria = sum_accuracy_total\n",
    "        print(\"{} --> {:.2f}%\".format(entity, percentual_acertos_modelo_por_categoria))\n",
    "        \n",
    "        accuracy_by_category[entity] = {str(percentual_acertos_modelo_por_categoria)}#para salvar no arquivo .txt com o percentual\n",
    "sum_accuracy = sum_correct / sum_total \n",
    "sum_accuracy_total = str(sum_accuracy) #para salvar no arquivo .txt com o percentual\n",
    "\n",
    "print(\"Acuracia Total (todas as categorias): {:.2f}%\".format(sum_accuracy))\n",
    "print('Total de entidades Rotulados Manualmente (referencia):', sum_total)\n",
    "print('Total de entidades Rotuladas Corretamente pelo modelo:', sum_correct)\n",
    "print('Total de entidades Rotuladas Incorretamente pelo modelo:', sum_incorretos_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salvados os resultados em um arquivo de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Salvando informações relevantes em um arquivo.txt\n",
    "nome_arquivo = '100%_metricas.txt'\n",
    "with open ('./ResultadosComDadosRotuladoAutomaticamente/'+nome_arquivo,\"w\", encoding='utf-8')  as output:\n",
    "    quantidadeDados = str(len(dataSetEntradaTeste))\n",
    "    output.write('Tamanho do dataset de teste: '+quantidadeDados)\n",
    "    output.write('\\n') \n",
    "    \n",
    "    output.write('\\n'+\"|CONFIGURAÇÕES UTILIZADAS PARA TREINAR O MODELO:|\"+'\\n')\n",
    "    epochsSize_String = str(epochsSize)\n",
    "    batchSize_String = str(batchSize)\n",
    "    drop_String = drop_size\n",
    "    output.write('Total de épocas: '+epochsSize_String+\"\\n\")\n",
    "    output.write('Total de batches: '+batchSize_String+\"\\n\")\n",
    "    output.write('Tamanho do Drop:')\n",
    "    output.write(str(drop_size))\n",
    "    output.write('\\n') \n",
    "                 \n",
    "    #Informações sobre treinamento do modelo\n",
    "    output.writelines('\\n'+\"|INFORMAÇÕES SOBRE TREINAMENTO DO MODELO:|\"+'\\n')\n",
    "    output.writelines('Perda acumulada do modelo (soma de todas as perdas): '+str(losses))\n",
    "    output.write('\\n')\n",
    "    output.writelines('Tempo gasto para treinar o modelo em segundos: '+str(tempoParaTreinarModelo))\n",
    "    output.write('\\n') \n",
    "    \n",
    "    #Métricas geradas pelo método 'evaluat', disponibilizado pelo próprio spaCy.\n",
    "    output.writelines('\\n'+\"|RESULTADOS DO MODELO PARA OS DADOS DO DATASET DE TESTE|\")    \n",
    "    output.writelines('\\n'+\"|Abaixo temos Precison, Recall, F-measure e Acuracia Geral:|\"+'\\n')\n",
    "    ents_p = \"Precisão Total: \"+str(round(calculatedBySpacy.get(\"ents_p\"),4))+'\\n'\n",
    "    ents_r = \"Revocação Total: \"+str(round(calculatedBySpacy.get(\"ents_r\"),4))+'\\n'\n",
    "    ents_f = \"F-measure Total: \"+str(round(calculatedBySpacy.get(\"ents_f\"),4))+'\\n'\n",
    "    ents_a = \"Acurácia Total: \"+str(round(sum_accuracy,4))+'\\n'\n",
    "    total_dados_com_rotulos = \"Total de entidades rotuladas existentes no dataset de teste: \"+str(round(sum_total,2))+'\\n'\n",
    "    total_dados_rotulados_corretamento = \"Total de entidades classificadas corretamente pelo modelo: \"+str(round(sum_correct,2))+'\\n'\n",
    "    total_dados_rotulados_incorretamentos = \"Total de entidades classificadas incorretamente pelo modelo: \"+str(round(sum_incorretos_total,2))+'\\n'\n",
    "    \n",
    "    output.write(ents_p),output.write(ents_r),output.write(ents_f),output.write(ents_a)\n",
    "    output.write(total_dados_com_rotulos),output.write(total_dados_rotulados_corretamento)\n",
    "    output.write(total_dados_rotulados_incorretamentos)\n",
    "    #output.writelines(';'.join(str(x) for x in (token_acc,token_p)))\n",
    "    \n",
    "    output.writelines('\\n'+\"|Abaixo temos Precision, Recall e F-measure para cada categoria:|\"+'\\n')\n",
    "    ents_per_type = calculatedBySpacy.get(\"ents_per_type\")\n",
    "    for key in ents_per_type:\n",
    "        ents_por_tipo = str(key)+str(ents_per_type[key])+'\\n'\n",
    "        output.write(ents_por_tipo) \n",
    "   \n",
    "    #Acuracia do modelo por categoria.\n",
    "    output.writelines('\\n'+\"|Abaixo temos a Acurácia para cada categoria:|\"+'\\n')\n",
    "    for key in accuracy_by_category:\n",
    "        entity_evaluation_accuracy = str(key)+str(accuracy_by_category[key])+'\\n'\n",
    "        output.write(entity_evaluation_accuracy)\n",
    "    \n",
    "    #Total de acertos e erros do modelo por categoria.    \n",
    "    output.writelines('\\n'+\"|Abaixo temos o total de acertos do modelo para cada categoria Versus o total que ele deveria classificar:|\"+'\\n')\n",
    "    for key in entity_evaluation:\n",
    "        if key != '':\n",
    "            acertos_erros = str(key)+str(entity_evaluation[key])+'\\n'\n",
    "            output.write(acertos_erros) \n",
    "            \n",
    "    #Parametros digitados no terminal pelo usuario\n",
    "    #output.write('\\n'+\"|ABAIXO TEMOS OS PARAMETROS INFORMADOS NO TEMRINAL:|\"+'\\n')\n",
    "    #output.writelines(';'.join(str(x) for x in (primeiraEntradaTerminal,segundaEntradaTerminal,terceiraEntradaTerminal,quartaEntradaTerminal,quintaEntradaTerminal)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dados Rotulados manualmente.\n",
    "\n",
    "nome_arquivo_2 = '100%_rotulacaomanual.txt'\n",
    "with open ('./ResultadosComDadosRotuladoManualmente/'+nome_arquivo_2,\"w\", encoding='utf-8')  as output:\n",
    "    for key in lista_rotulacaomanual_1:\n",
    "        key = str(key)+'\\n'\n",
    "        output.write(key)\n",
    "\n",
    "#Dados Rotulados atomaticamente.\n",
    "\n",
    "nome_arquivo_3 = '100%_rotulacaoautomatical.txt'\n",
    "with open ('./ResultadosComDadosRotuladoAutomaticamente/'+nome_arquivo_3,\"w\", encoding='utf-8')  as output:\n",
    "    for key in lista_rotulacaomanual_2:\n",
    "        key = str(key)+'\\n'\n",
    "        output.write(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
